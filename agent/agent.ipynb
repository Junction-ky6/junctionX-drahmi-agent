{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnQQxOJ0-lEE"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J0Uf7fxn-iNJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "import torch.distributions as dist\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Category():\n",
        "    def __init__(self, name, depense:bool):\n",
        "        self.name = name\n",
        "        self.ratio = 0.0\n",
        "        self.actual_ratio = 0.0\n",
        "        self.user_satisfaction = 0.0\n",
        "        if depense:\n",
        "            self.is_depense = True\n",
        "        else:\n",
        "            self.is_depense = False\n",
        "        \n",
        "    def benefit(self):\n",
        "        if self.is_depense:\n",
        "            return self.ratio - self.actual_ratio\n",
        "        else:\n",
        "            return self.actual_ratio - self.ratio\n",
        "        \n",
        "    def satisafaction(self):\n",
        "        return self.user_satisfaction\n",
        "        \n",
        "    def reset(self):\n",
        "        self.ratio = 0.0\n",
        "        self.actual_ratio = 0.0\n",
        "        self.user_satisfaction = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Goal():\n",
        "    def __init__(self, goal):\n",
        "        self.goal = goal\n",
        "        self.money = 0.0\n",
        "        \n",
        "    def add(self, cach):\n",
        "        self.money = cach\n",
        "\n",
        "    def achievement(self):\n",
        "        if self.money >= self.goal:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return self.money / self.goal\n",
        "        \n",
        "    def reset(self):\n",
        "        self.money = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Investment():\n",
        "    def __init__(self):\n",
        "        self.income = 0\n",
        "        self.outcome = 0\n",
        "        \n",
        "    def is_running(self):\n",
        "        return self.outcome > 0\n",
        "    \n",
        "    def profit(self):\n",
        "        if self.is_running():\n",
        "            return self.income / self.outcome\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def reset(self):\n",
        "        self.income = 0\n",
        "        self.outcome = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRiuKp2t-sXz"
      },
      "source": [
        "# Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Action():\n",
        "    def __init__(self, categories_ratios, goal_ratio, investment_ratio):\n",
        "        self.categories_ratios = categories_ratios\n",
        "        self.goal_ratio = goal_ratio\n",
        "        self.investment_ratio = investment_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Simulation():\n",
        "    def __init__(self, money, actual_categories_ratios, user_satisfactions, investment_icome):\n",
        "        self.actual_categories_ratios = actual_categories_ratios\n",
        "        self.user_satisfactions = user_satisfactions\n",
        "        self.investment_income = investment_icome\n",
        "        self.money = money"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A7jwMVK8-w4p"
      },
      "outputs": [],
      "source": [
        "class Env():\n",
        "  def __init__(self, initial_money, categories: list[Category], goal: Goal, investment: Investment):\n",
        "    self.money = initial_money\n",
        "    self.categories = categories\n",
        "    self.goal = goal\n",
        "    self.investment = investment\n",
        "    \n",
        "  def add(self, cach_in):\n",
        "    self.money += cach_in\n",
        "    \n",
        "  def observation(self):\n",
        "    return np.array([self.money] + [c.actual_ratio for c in self.categories] + [c.user_satisfaction for c in self.categories] + [self.goal.achievement(), self.investment.profit()])\n",
        "  \n",
        "  def update(self, action: Action):\n",
        "    for i in range(len(self.categories)):\n",
        "      self.categories[i].ratio = action.categories_ratios[i]  \n",
        "    self.goal.add(action.goal_ratio * self.money)\n",
        "    self.investment.outcome += action.investment_ratio * self.money\n",
        "    \n",
        "  def simulate(self, simulation: Simulation):\n",
        "    for i, category in enumerate(self.categories):\n",
        "      category.actual_ratio = simulation.actual_categories_ratios[i]\n",
        "      category.user_satisfaction = simulation.user_satisfactions[i]\n",
        "    self.investment.income += simulation.investment_income\n",
        "    \n",
        "  def reward(self):\n",
        "    return sum([c.benefit() for c in self.categories] + [c.satisafaction() for c in self.categories] + [self.goal.achievement(), self.investment.profit()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_simulations(path):\n",
        "    with open(path) as f:\n",
        "        sims =  json.load(f)\n",
        "        \n",
        "    simulations = []\n",
        "    for s in sims:\n",
        "        for k,v in s.items():\n",
        "            if isinstance(v, list):\n",
        "                s[k] = [float(x) for x in v]\n",
        "            else:\n",
        "                s[k] = float(v)\n",
        "        simulations.append(Simulation(**s))\n",
        "    return simulations\n",
        "\n",
        "def get_simulation(simulations, iter):\n",
        "    return simulations[iter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_action(action):\n",
        "    return Action(categories_ratios=action[:5], goal_ratio=action[5], investment_ratio=action[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_tensor(obs, acts, rewards, log_probs, values, advantages):\n",
        "    return torch.tensor(obs, dtype=torch.float), torch.tensor(acts, dtype=torch.float), torch.tensor(rewards, dtype=torch.float), torch.tensor(log_probs, dtype=torch.float), torch.tensor(values, dtype=torch.float), torch.tensor(advantages, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batches(data, batch_size):\n",
        "    batches = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batches.append(data[i:i+batch_size])\n",
        "    return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "Simulations = load_simulations(\"../data/simulations.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "investment = Investment()\n",
        "goal = Goal(120)\n",
        "\n",
        "billing = Category(\"Billing\", True)\n",
        "dept = Category(\"Dept\", True)\n",
        "purchase = Category(\"Purchase\", True)\n",
        "entertainment = Category(\"Entertainment\", True)\n",
        "saving = Category(\"Saving\", False)\n",
        "\n",
        "categories = [billing, dept, purchase, entertainment, saving]\n",
        "\n",
        "action = Action([0.1, 0.1, 0.1, 0.1, 0.2], 0.1, 0.3)\n",
        "\n",
        "env = Env(200, categories, goal, investment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(Simulations)):\n",
        "    # print(\"OBSERVATION\", [round(x, 2) for x in env.observation()])\n",
        "    env.update(action)\n",
        "    env.simulate(get_simulation(Simulations, i))\n",
        "    # print(\"OBSERVATION\", [round(x, 2) for x in env.observation()])\n",
        "    # print(\"REWARD\", round(env.reward(), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, obs_space_size, action_space_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.shared_layers = nn.Sequential(\n",
        "            nn.Linear(obs_space_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.policy_mean = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_space_size))  # Output layer for the mean of the action distribution\n",
        "\n",
        "        self.policy_std = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_space_size))  # Output layer for the standard deviation of the action distribution\n",
        "\n",
        "        self.value_layers = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1))\n",
        "\n",
        "    def value(self, obs):\n",
        "        z = self.shared_layers(obs)\n",
        "        value = self.value_layers(z)\n",
        "        return value\n",
        "\n",
        "    def policy(self, obs):\n",
        "        z = self.shared_layers(obs)\n",
        "        mean = self.policy_mean(z)\n",
        "        std = torch.exp(self.policy_std(z))  # Using the exponential function to ensure std is positive\n",
        "        return mean, std\n",
        "\n",
        "    def forward(self, obs):\n",
        "        z = self.shared_layers(obs)\n",
        "        mean = self.policy_mean(z)\n",
        "        std = torch.exp(self.policy_std(z))\n",
        "        value = self.value_layers(z)\n",
        "        return mean, std, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork:\n",
        "    def sample(self, ac_network, obs_tensor):\n",
        "        mean, std = ac_network.policy(obs_tensor)\n",
        "        action = torch.normal(mean, std)\n",
        "        log_prob = torch.distributions.Normal(mean, std).log_prob(action)\n",
        "        return action, log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOTrainer():\n",
        "    def __init__(self,\n",
        "                actor_critic,\n",
        "                ppo_clip_val=0.2,\n",
        "                target_kl_div=0.02,\n",
        "                max_policy_train_iters=4,\n",
        "                value_train_iters=4,\n",
        "                policy_lr=1e-5,\n",
        "                value_lr=1e-5):\n",
        "        self.ac = actor_critic\n",
        "        self.ppo_clip_val = ppo_clip_val\n",
        "        self.target_kl_div = target_kl_div\n",
        "        self.max_policy_train_iters = max_policy_train_iters\n",
        "        self.value_train_iters = value_train_iters\n",
        "\n",
        "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "            list(self.ac.policy_mean.parameters()) + list(self.ac.policy_std.parameters())\n",
        "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
        "\n",
        "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "            list(self.ac.value_layers.parameters())\n",
        "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
        "\n",
        "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
        "        for _ in range(self.max_policy_train_iters):\n",
        "            self.policy_optim.zero_grad()\n",
        "\n",
        "            # 1. Compute new action logits and log probabilities\n",
        "            new_mean, new_std = self.ac.policy(obs)\n",
        "            new_dist = dist.Normal(new_mean, new_std)\n",
        "            new_log_probs = new_dist.log_prob(acts)\n",
        "\n",
        "            # 2. Compute the policy ratio and apply clipping\n",
        "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
        "            \n",
        "            # 3. Compute PPO loss\n",
        "            # Make sure tensors have the same shape\n",
        "            clipped_ratio = clipped_ratio.unsqueeze(1)\n",
        "            gaes = gaes.unsqueeze(1) \n",
        "            policy_ratio = policy_ratio.unsqueeze(1)\n",
        "            \n",
        "            clipped_loss = torch.min(clipped_ratio * gaes, policy_ratio * gaes)\n",
        "            policy_loss = -clipped_loss.mean()\n",
        "\n",
        "            # 4. Backpropagate and update the policy network\n",
        "            policy_loss.backward()\n",
        "            self.policy_optim.step()\n",
        "\n",
        "            # 5. Save the new mean and standard deviation as the old values\n",
        "            old_mean, old_std = new_mean, new_std\n",
        "\n",
        "            # 6. Calculate the KL divergence between old and new policies\n",
        "            kl_div = torch.distributions.kl.kl_divergence(\n",
        "                dist.Normal(old_mean, old_std), new_dist).mean()\n",
        "\n",
        "            # 7. Check if the KL divergence is within the target range\n",
        "            if kl_div >= self.target_kl_div:\n",
        "                break\n",
        "\n",
        "\n",
        "    def train_value(self, obs, returns):\n",
        "        for _ in range(self.value_train_iters):\n",
        "            self.value_optim.zero_grad()\n",
        "\n",
        "            # 1. Compute predicted values from the value network\n",
        "            values = self.ac.value(obs)\n",
        "\n",
        "            # 2. Compute the value loss (mean squared error)\n",
        "            value_loss = (torch.tensor(returns, dtype=torch.float) - values) ** 2  \n",
        "            value_loss = value_loss.mean()\n",
        "\n",
        "            # 3. Backpropagate and update the value network\n",
        "            value_loss.backward()\n",
        "            self.value_optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discount_rewards(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Return discounted rewards based on the given rewards and gamma param.\n",
        "    \"\"\"\n",
        "    new_rewards = [float(rewards[-1])]\n",
        "    for i in reversed(range(len(rewards)-1)):\n",
        "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
        "    return np.array(new_rewards[::-1])\n",
        "\n",
        "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
        "    \"\"\"\n",
        "    Return the General Advantage Estimates from the given rewards and values.\n",
        "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
        "    \"\"\"\n",
        "    next_values = np.concatenate([values[1:], [0]])\n",
        "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
        "\n",
        "    gaes = [deltas[-1]]\n",
        "    for i in reversed(range(len(deltas)-1)):\n",
        "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
        "\n",
        "    return np.array(gaes[::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_trajectories(env, ac_network, p_network, num_trajectories, simulations):\n",
        "    all_observations = []\n",
        "    all_actions = []\n",
        "    all_rewards = []\n",
        "    all_log_probs = []\n",
        "    all_values = []\n",
        "\n",
        "    for _ in range(num_trajectories):\n",
        "        observations = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        values = []\n",
        "\n",
        "        obs = env.observation()\n",
        "\n",
        "        for i, simulation in enumerate(simulations):\n",
        "            # Sample an action from the policy network\n",
        "            with torch.no_grad():\n",
        "                obs_tensor = torch.FloatTensor(obs)\n",
        "                action, log_prob = p_network.sample(ac_network, obs_tensor)\n",
        "                value = ac_network.value(obs_tensor)\n",
        "\n",
        "            # Convert the action to a NumPy array\n",
        "            action_np = action.numpy()\n",
        "            action = get_action(action)\n",
        "\n",
        "            env.update(action)\n",
        "            env.simulate(simulation)\n",
        "            \n",
        "            next_obs, reward, = env.observation(), env.reward()\n",
        "\n",
        "            # Store the results\n",
        "            observations.append(obs)\n",
        "            actions.append(action_np)\n",
        "            rewards.append(reward)\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "        all_observations.extend(observations)\n",
        "        all_actions.extend(actions)\n",
        "        all_rewards.extend(rewards)\n",
        "        all_log_probs.extend(log_probs)\n",
        "        all_values.extend(values)\n",
        "\n",
        "    return (\n",
        "        np.array(all_observations),\n",
        "        np.array(all_actions),\n",
        "        np.array(all_rewards),\n",
        "        np.array(all_log_probs),\n",
        "        np.array(all_values)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "simulations = load_simulations(\"../data/record.json\")\n",
        "batches = get_batches(simulations, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "obs_space_size = 13 \n",
        "action_space_size = 7  \n",
        "\n",
        "ac_network = ActorCriticNetwork(obs_space_size, action_space_size)\n",
        "p_network = PolicyNetwork()\n",
        "ppo_trainer = PPOTrainer(ac_network, policy_lr=1e-10, value_lr=1e-10)\n",
        "\n",
        "num_epochs = 10 \n",
        "\n",
        "num_trajectories = 2\n",
        "reward_rec = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "    print(f\"[EPOCH] {epoch}\")\n",
        "    for i, batch in enumerate(batches):\n",
        "        obs, acts, rewards, log_probs, values = collect_trajectories(env, ac_network, p_network, num_trajectories, batch)\n",
        "        returns = discount_rewards(rewards)\n",
        "        values = values.reshape(-1)\n",
        "        advantages = calculate_gaes(rewards, values)\n",
        "        obs, acts, returns, log_probs, values, advantages = to_tensor(obs, acts, returns, log_probs, values, advantages)\n",
        "        \n",
        "        ppo_trainer.train_policy(obs, acts, log_probs, advantages)\n",
        "        ppo_trainer.train_value(obs, returns)\n",
        "        \n",
        "        reward_rec.append(rewards.mean())\n",
        "        if i % 10 == 0:\n",
        "            print(f\"[BATCH] {i} | Reward: {rewards.mean():.2f}\")\n",
        "        \n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch: {epoch} | Rewards: {rewards.mean():.2f} | Returns: {returns.mean():.2f} | Advantages: {advantages.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(ac_network.state_dict(), \"ppo_actor_critic.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
